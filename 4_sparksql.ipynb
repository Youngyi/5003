{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext(\"local\",'app')\n",
    "spark = SparkSession.builder.appName('name').config('spark.sql.shuffle.partitions',10).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(age=11, name='Alice')\n",
      "Alice 11\n",
      "Alice 11\n",
      "<built-in method count of Row object at 0x1102818f0>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "row = Row(name=\"Alice\", age=11)\n",
    "print row\n",
    "print row['name'], row['age']\n",
    "print row.name, row.age\n",
    "\n",
    "row = Row(name=\"Alice\", age=11, count=1)\n",
    "print row.count\n",
    "print row['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data file at https://www.cse.ust.hk/msbd/data/building.csv\n",
    "df = spark.read.csv('../data/building.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+------------+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|     Country|\n",
      "+----------+-----------+-----------+-----------+------------+\n",
      "|         1|         M1|         25|     AC1000|         USA|\n",
      "|         2|         M2|         27|     FN39TG|      France|\n",
      "|         3|         M3|         28|     JDNS77|      Brazil|\n",
      "|         4|         M4|         17|     GG1919|     Finland|\n",
      "|         5|         M5|          3|    ACMAX22|   Hong Kong|\n",
      "|         6|         M6|          9|     AC1000|   Singapore|\n",
      "|         7|         M7|         13|     FN39TG|South Africa|\n",
      "|         8|         M8|         25|     JDNS77|   Australia|\n",
      "|         9|         M9|         11|     GG1919|      Mexico|\n",
      "|        10|        M10|         23|    ACMAX22|       China|\n",
      "|        11|        M11|         14|     AC1000|     Belgium|\n",
      "|        12|        M12|         26|     FN39TG|     Finland|\n",
      "|        13|        M13|         25|     JDNS77|Saudi Arabia|\n",
      "|        14|        M14|         17|     GG1919|     Germany|\n",
      "|        15|        M15|         19|    ACMAX22|      Israel|\n",
      "|        16|        M16|         23|     AC1000|      Turkey|\n",
      "|        17|        M17|         11|     FN39TG|       Egypt|\n",
      "|        18|        M18|         25|     JDNS77|   Indonesia|\n",
      "|        19|        M19|         14|     GG1919|      Canada|\n",
      "|        20|        M20|         19|    ACMAX22|   Argentina|\n",
      "+----------+-----------+-----------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the content of the dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BuildingID: integer (nullable = true)\n",
      " |-- BuildingMgr: string (nullable = true)\n",
      " |-- BuildingAge: integer (nullable = true)\n",
      " |-- HVACproduct: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the dataframe schema in a tree format\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(BuildingID=1, BuildingMgr=u'M1', BuildingAge=25, HVACproduct=u'AC1000', Country=u'USA')]\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD from the dataframe\n",
    "dfrdd = df.rdd\n",
    "print dfrdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|BuildingID|     Country|\n",
      "+----------+------------+\n",
      "|         1|         USA|\n",
      "|         2|      France|\n",
      "|         3|      Brazil|\n",
      "|         4|     Finland|\n",
      "|         5|   Hong Kong|\n",
      "|         6|   Singapore|\n",
      "|         7|South Africa|\n",
      "|         8|   Australia|\n",
      "|         9|      Mexico|\n",
      "|        10|       China|\n",
      "|        11|     Belgium|\n",
      "|        12|     Finland|\n",
      "|        13|Saudi Arabia|\n",
      "|        14|     Germany|\n",
      "|        15|      Israel|\n",
      "|        16|      Turkey|\n",
      "|        17|       Egypt|\n",
      "|        18|   Indonesia|\n",
      "|        19|      Canada|\n",
      "|        20|   Argentina|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve specific columns from the dataframe\n",
    "df.select('BuildingID', 'Country').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+-------+---+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|Country| OK|\n",
      "+----------+-----------+-----------+-----------+-------+---+\n",
      "|         1|         M1|         25|     AC1000|    USA| OK|\n",
      "+----------+-----------+-----------+-----------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df.where(\"Country='USA'\").select('*', lit('OK')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|HVACProduct|count|\n",
      "+-----------+-----+\n",
      "|     GG1919|    4|\n",
      "|    ACMAX22|    4|\n",
      "|     AC1000|    4|\n",
      "|     FN39TG|    4|\n",
      "|     JDNS77|    4|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use GroupBy clause with dataframe \n",
    "df.groupBy('HVACProduct').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewriting SQL with DataFrame API\n",
    "\n",
    "The data files have been put to a public blob container, which can be accessed as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data from csv files\n",
    "# Data files at https://www.cse.ust.hk/msbd5003/data\n",
    "dfCustomer = spark.read.csv('../data/Customer.csv', header=True, inferSchema=True)\n",
    "dfDetail = spark.read.csv('../data/SalesOrderDetail.csv', header=True, inferSchema=True)\n",
    "dfHeader = spark.read.csv('../data/SalesOrderHeader.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- NameStyle: integer (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- FirstName: string (nullable = true)\n",
      " |-- MiddleName: string (nullable = true)\n",
      " |-- LastName: string (nullable = true)\n",
      " |-- Suffix: string (nullable = true)\n",
      " |-- CompanyName: string (nullable = true)\n",
      " |-- SalesPerson: string (nullable = true)\n",
      " |-- EmailAddress: string (nullable = true)\n",
      " |-- Phone: string (nullable = true)\n",
      " |-- PasswordHash: string (nullable = true)\n",
      " |-- PasswordSalt: string (nullable = true)\n",
      " |-- rowguid: string (nullable = true)\n",
      " |-- ModifiedDate: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfCustomer.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SalesOrderID: integer (nullable = true)\n",
      " |-- SalesOrderDetailID: integer (nullable = true)\n",
      " |-- OrderQty: integer (nullable = true)\n",
      " |-- ProductID: integer (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- UnitPriceDiscount: double (nullable = true)\n",
      " |-- LineTotal: double (nullable = true)\n",
      " |-- rowguid: string (nullable = true)\n",
      " |-- ModifiedDate: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfDetail.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[110] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numPartitions = 10\n",
    "lines = sc.textFile('adj_noun_pairs.txt', numPartitions)\n",
    "pairs = lines.map(lambda l: tuple(l.split())).filter(lambda p: len(p)==2)\n",
    "pairs.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'indic', u'tamil')\n"
     ]
    }
   ],
   "source": [
    "pair_freqs = pairs.map(lambda p: (p,1)).reduceByKey(lambda f1, f2: f1 + f2)\n",
    "pair_freqs.cache()\n",
    "\n",
    "sort_pair_freqs=pair_freqs.sortBy(lambda w:w[1],False)\n",
    "sort_pair_freqs.cache()\n",
    "\n",
    "with_index_pair=sort_pair_freqs.zipWithIndex()\n",
    "with_index_pair.cache()\n",
    "\n",
    "amount_pair=with_index_pair.count()\n",
    "median_number=(amount_pair+1)/2\n",
    "\n",
    "temp_answer=with_index_pair.filter(lambda w: w[1]==(median_number-1))\n",
    "temp_answer.cache()\n",
    "median_pair=temp_answer.collect()[0][0][0]\n",
    "print median_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'indic', u'tamil')\n"
     ]
    }
   ],
   "source": [
    "pair_freqs = pairs.map(lambda p: (p,1)).reduceByKey(lambda f1, f2: f1 + f2)\n",
    "pair_freqs.cache()\n",
    "\n",
    "sort_pair_freqs=pair_freqs.sortBy(lambda w:w[1],False)\n",
    "sort_pair_freqs.cache()\n",
    "\n",
    "with_index_pair=sort_pair_freqs.zipWithIndex()\n",
    "with_index_pair.cache()\n",
    "\n",
    "amount_pair=with_index_pair.count()\n",
    "median_number=(amount_pair+1)/2\n",
    "\n",
    "temp_answer=with_index_pair.filter(lambda w: w[1]==(median_number-1))\n",
    "temp_answer.cache()\n",
    "median_pair=temp_answer.collect()[0][0][0]\n",
    "print median_pair\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(29736, 89869.27631400003), (30050, 79589.61602399996), (29546, 74160.228), (29957, 65683.367986), (29796, 65123.46341800001), (29929, 59894.209199999976), (29932, 53248.69200000002), (29660, 47848.02600000001), (29938, 34118.5356), (29485, 33319.98600000001), (30113, 29923.007999999998), (29922, 28950.678108), (29653, 11528.844000000001), (29877, 10585.05), (29531, 5533.8689079999995), (30027, 2847.4079999999994), (29975, 2777.1430760000003), (29982, 2527.128), (30102, 1884.3948), (29568, 1856.2068), (29638, 1732.8899999999999), (30019, 1712.946), (30089, 926.916), (29644, 858.9)]\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "details=dfDetail.select('*', (dfDetail.UnitPrice * dfDetail. OrderQty\\\n",
    "* (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    ".groupBy('SalesOrderID').sum('netprice') \\\n",
    ".withColumnRenamed('sum(netprice)', 'TotalPrice')\n",
    "\n",
    "salesOrder_condition=dfHeader.join(details,'SalesOrderID','left_outer')\n",
    "\n",
    "\n",
    "temp_result=dfCustomer.join(salesOrder_condition, \"CustomerID\",'left_outer')\\\n",
    ".groupBy('CustomerID').sum('TotalPrice').na.fill(0).withColumnRenamed('sum(TotalPrice)','Total_Price')\n",
    "\n",
    "total_price=temp_result.rdd.map(lambda row : row['Total_Price'])\n",
    "\n",
    "total_price.cache()\n",
    "average_total_price=total_price.reduce(lambda x,y:x+y)/float(total_price.count())\n",
    "\n",
    "candidates=temp_result.filter(temp_result[\"Total_Price\"] >average_total_price)\\\n",
    ".orderBy(\"Total_price\",ascending=False)\n",
    "\n",
    "valuable_candidates=candidates.rdd.map(lambda row:(row['CustomerID'],row['Total_Price'])).collect()\n",
    "print valuable_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u\"cannot resolve '``' given input columns: [CustomerID, Total_Price];;\\n'Aggregate ['], [', avg(Total_Price#614) AS avg(Total_Price)#666]\\n+- Project [CustomerID#609, sum(TotalPrice)#610 AS Total_Price#614]\\n   +- Project [coalesce(CustomerID#12, cast(0.0 as int)) AS CustomerID#609, coalesce(nanvl(sum(TotalPrice)#602, cast(null as double)), cast(0.0 as double)) AS sum(TotalPrice)#610]\\n      +- Aggregate [CustomerID#12], [CustomerID#12, sum(TotalPrice#497) AS sum(TotalPrice)#602]\\n         +- Project [CustomerID#12, NameStyle#13, Title#14, FirstName#15, MiddleName#16, LastName#17, Suffix#18, CompanyName#19, SalesPerson#20, EmailAddress#21, Phone#22, PasswordHash#23, PasswordSalt#24, rowguid#25, ModifiedDate#26, SalesOrderID#86, RevisionNumber#87, OrderDate#88, DueDate#89, ShipDate#90, Status#91, OnlineOrderFlag#92, SalesOrderNumber#93, PurchaseOrderNumber#94, ... 13 more fields]\\n            +- Join LeftOuter, (CustomerID#12 = CustomerID#96)\\n               :- Relation[CustomerID#12,NameStyle#13,Title#14,FirstName#15,MiddleName#16,LastName#17,Suffix#18,CompanyName#19,SalesPerson#20,EmailAddress#21,Phone#22,PasswordHash#23,PasswordSalt#24,rowguid#25,ModifiedDate#26] csv\\n               +- Project [SalesOrderID#86, RevisionNumber#87, OrderDate#88, DueDate#89, ShipDate#90, Status#91, OnlineOrderFlag#92, SalesOrderNumber#93, PurchaseOrderNumber#94, AccountNumber#95, CustomerID#96, ShipToAddressID#97, BillToAddressID#98, ShipMethod#99, CreditCardApprovalCode#100, SubTotal#101, TaxAmt#102, Freight#103, TotalDue#104, Comment#105, rowguid#106, ModifiedDate#107, TotalPrice#497]\\n                  +- Join LeftOuter, (SalesOrderID#86 = SalesOrderID#55)\\n                     :- Relation[SalesOrderID#86,RevisionNumber#87,OrderDate#88,DueDate#89,ShipDate#90,Status#91,OnlineOrderFlag#92,SalesOrderNumber#93,PurchaseOrderNumber#94,AccountNumber#95,CustomerID#96,ShipToAddressID#97,BillToAddressID#98,ShipMethod#99,CreditCardApprovalCode#100,SubTotal#101,TaxAmt#102,Freight#103,TotalDue#104,Comment#105,rowguid#106,ModifiedDate#107] csv\\n                     +- Project [SalesOrderID#55, sum(netprice)#493 AS TotalPrice#497]\\n                        +- Aggregate [SalesOrderID#55], [SalesOrderID#55, sum(netprice#469) AS sum(netprice)#493]\\n                           +- Project [SalesOrderID#55, SalesOrderDetailID#56, OrderQty#57, ProductID#58, UnitPrice#59, UnitPriceDiscount#60, LineTotal#61, rowguid#62, ModifiedDate#63, ((UnitPrice#59 * cast(OrderQty#57 as double)) * (cast(1 as double) - UnitPriceDiscount#60)) AS netprice#469]\\n                              +- Relation[SalesOrderID#55,SalesOrderDetailID#56,OrderQty#57,ProductID#58,UnitPrice#59,UnitPriceDiscount#60,LineTotal#61,rowguid#62,ModifiedDate#63] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-24f5d3c2841f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtemp_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total_Price\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/andy/anaconda2/lib/python2.7/site-packages/pyspark/sql/group.pyc\u001b[0m in \u001b[0;36m_api\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/andy/anaconda2/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/andy/anaconda2/lib/python2.7/site-packages/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u\"cannot resolve '``' given input columns: [CustomerID, Total_Price];;\\n'Aggregate ['], [', avg(Total_Price#614) AS avg(Total_Price)#666]\\n+- Project [CustomerID#609, sum(TotalPrice)#610 AS Total_Price#614]\\n   +- Project [coalesce(CustomerID#12, cast(0.0 as int)) AS CustomerID#609, coalesce(nanvl(sum(TotalPrice)#602, cast(null as double)), cast(0.0 as double)) AS sum(TotalPrice)#610]\\n      +- Aggregate [CustomerID#12], [CustomerID#12, sum(TotalPrice#497) AS sum(TotalPrice)#602]\\n         +- Project [CustomerID#12, NameStyle#13, Title#14, FirstName#15, MiddleName#16, LastName#17, Suffix#18, CompanyName#19, SalesPerson#20, EmailAddress#21, Phone#22, PasswordHash#23, PasswordSalt#24, rowguid#25, ModifiedDate#26, SalesOrderID#86, RevisionNumber#87, OrderDate#88, DueDate#89, ShipDate#90, Status#91, OnlineOrderFlag#92, SalesOrderNumber#93, PurchaseOrderNumber#94, ... 13 more fields]\\n            +- Join LeftOuter, (CustomerID#12 = CustomerID#96)\\n               :- Relation[CustomerID#12,NameStyle#13,Title#14,FirstName#15,MiddleName#16,LastName#17,Suffix#18,CompanyName#19,SalesPerson#20,EmailAddress#21,Phone#22,PasswordHash#23,PasswordSalt#24,rowguid#25,ModifiedDate#26] csv\\n               +- Project [SalesOrderID#86, RevisionNumber#87, OrderDate#88, DueDate#89, ShipDate#90, Status#91, OnlineOrderFlag#92, SalesOrderNumber#93, PurchaseOrderNumber#94, AccountNumber#95, CustomerID#96, ShipToAddressID#97, BillToAddressID#98, ShipMethod#99, CreditCardApprovalCode#100, SubTotal#101, TaxAmt#102, Freight#103, TotalDue#104, Comment#105, rowguid#106, ModifiedDate#107, TotalPrice#497]\\n                  +- Join LeftOuter, (SalesOrderID#86 = SalesOrderID#55)\\n                     :- Relation[SalesOrderID#86,RevisionNumber#87,OrderDate#88,DueDate#89,ShipDate#90,Status#91,OnlineOrderFlag#92,SalesOrderNumber#93,PurchaseOrderNumber#94,AccountNumber#95,CustomerID#96,ShipToAddressID#97,BillToAddressID#98,ShipMethod#99,CreditCardApprovalCode#100,SubTotal#101,TaxAmt#102,Freight#103,TotalDue#104,Comment#105,rowguid#106,ModifiedDate#107] csv\\n                     +- Project [SalesOrderID#55, sum(netprice)#493 AS TotalPrice#497]\\n                        +- Aggregate [SalesOrderID#55], [SalesOrderID#55, sum(netprice#469) AS sum(netprice)#493]\\n                           +- Project [SalesOrderID#55, SalesOrderDetailID#56, OrderQty#57, ProductID#58, UnitPrice#59, UnitPriceDiscount#60, LineTotal#61, rowguid#62, ModifiedDate#63, ((UnitPrice#59 * cast(OrderQty#57 as double)) * (cast(1 as double) - UnitPriceDiscount#60)) AS netprice#469]\\n                              +- Relation[SalesOrderID#55,SalesOrderDetailID#56,OrderQty#57,ProductID#58,UnitPrice#59,UnitPriceDiscount#60,LineTotal#61,rowguid#62,ModifiedDate#63] csv\\n\""
     ]
    }
   ],
   "source": [
    "temp_result.groupBy(\"\").avg(\"Total_Price\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|CustomerID|Total_Price|\n",
      "+----------+-----------+\n",
      "|        22|        0.0|\n",
      "|        60|        0.0|\n",
      "|        92|        0.0|\n",
      "|        95|        0.0|\n",
      "|       128|        0.0|\n",
      "|       148|        0.0|\n",
      "|       155|        0.0|\n",
      "|       167|        0.0|\n",
      "|       168|        0.0|\n",
      "|       205|        0.0|\n",
      "|       209|        0.0|\n",
      "|       288|        0.0|\n",
      "|       292|        0.0|\n",
      "|       304|        0.0|\n",
      "|       306|        0.0|\n",
      "|       307|        0.0|\n",
      "|       326|        0.0|\n",
      "|       327|        0.0|\n",
      "|       330|        0.0|\n",
      "|       371|        0.0|\n",
      "+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "836.706201957\n"
     ]
    }
   ],
   "source": [
    "details=dfDetail.select('*', (dfDetail.UnitPrice * dfDetail. OrderQty\\\n",
    "         * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    "        .groupBy('SalesOrderID').sum('netprice') \\\n",
    "        .withColumnRenamed('sum(netprice)', 'TotalPrice')\n",
    "      \n",
    "salesOrder_condition=dfHeader.join(details,'SalesOrderID','left_outer')\n",
    "temp_result=dfCustomer.join(salesOrder_condition, \"CustomerID\",'left_outer')\\\n",
    "    .groupBy('CustomerID').sum('TotalPrice').na.fill(0).withColumnRenamed('sum(TotalPrice)','Total_Price')\n",
    "temp_result.show()   \n",
    "total_price=temp_result.rdd.map(lambda row : row['Total_Price'])\n",
    "total_price.cache()\n",
    "average_total_price=total_price.reduce(lambda x,y:x+y)/float(total_price.count())\n",
    "print average_total_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29736, 30050, 29546, 29957, 29796, 29929, 29932, 29660, 29938, 29485, 30113, 29922, 29653, 29877, 29531, 30027, 29975, 29982, 30102, 29568, 29638, 30019, 30089, 29644]\n"
     ]
    }
   ],
   "source": [
    "x=average_total_price\n",
    "candidates=temp_result.filter(temp_result[\"Total_Price\"] >average_total_price)\\\n",
    "    .orderBy(\"Total_price\",ascending=False)    \n",
    "\n",
    "valuable_candidates=candidates.rdd.map(lambda row:row['CustomerID']).collect()\n",
    "print valuable_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+---------+\n",
      "|ProductID|Name                         |ListPrice|\n",
      "+---------+-----------------------------+---------+\n",
      "|680      |HL Road Frame - Black, 58    |1431.5   |\n",
      "|708      |Sport-100 Helmet, Black      |34.99    |\n",
      "|722      |LL Road Frame - Black, 58    |337.22   |\n",
      "|723      |LL Road Frame - Black, 60    |337.22   |\n",
      "|724      |LL Road Frame - Black, 62    |337.22   |\n",
      "|736      |LL Road Frame - Black, 44    |337.22   |\n",
      "|737      |LL Road Frame - Black, 48    |337.22   |\n",
      "|738      |LL Road Frame - Black, 52    |337.22   |\n",
      "|743      |HL Mountain Frame - Black, 42|1349.6   |\n",
      "|744      |HL Mountain Frame - Black, 44|1349.6   |\n",
      "|745      |HL Mountain Frame - Black, 48|1349.6   |\n",
      "|746      |HL Mountain Frame - Black, 46|1349.6   |\n",
      "|747      |HL Mountain Frame - Black, 38|1349.6   |\n",
      "|765      |Road-650 Black, 58           |782.99   |\n",
      "|766      |Road-650 Black, 60           |782.99   |\n",
      "|767      |Road-650 Black, 62           |782.99   |\n",
      "|768      |Road-650 Black, 44           |782.99   |\n",
      "|769      |Road-650 Black, 48           |782.99   |\n",
      "|770      |Road-650 Black, 52           |782.99   |\n",
      "|775      |Mountain-100 Black, 38       |3374.99  |\n",
      "+---------+-----------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SELECT ProductID, Name, ListPrice \n",
    "# FROM Product \n",
    "# WHERE Color = 'black'\n",
    "# pay attention to the grammer to write the filter\n",
    "dfProduct.filter(\"Color = 'Black'\")\\  \n",
    "         .select('ProductID', 'Name', 'ListPrice')\\\n",
    "         .show(truncate=False)  # do not hide any information in each data column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+-----------+\n",
      "|ProductID|Name                         |DoublePrice|\n",
      "+---------+-----------------------------+-----------+\n",
      "|680      |HL Road Frame - Black, 58    |2863.0     |\n",
      "|708      |Sport-100 Helmet, Black      |69.98      |\n",
      "|722      |LL Road Frame - Black, 58    |674.44     |\n",
      "|723      |LL Road Frame - Black, 60    |674.44     |\n",
      "|724      |LL Road Frame - Black, 62    |674.44     |\n",
      "|736      |LL Road Frame - Black, 44    |674.44     |\n",
      "|737      |LL Road Frame - Black, 48    |674.44     |\n",
      "|738      |LL Road Frame - Black, 52    |674.44     |\n",
      "|743      |HL Mountain Frame - Black, 42|2699.2     |\n",
      "|744      |HL Mountain Frame - Black, 44|2699.2     |\n",
      "|745      |HL Mountain Frame - Black, 48|2699.2     |\n",
      "|746      |HL Mountain Frame - Black, 46|2699.2     |\n",
      "|747      |HL Mountain Frame - Black, 38|2699.2     |\n",
      "|765      |Road-650 Black, 58           |1565.98    |\n",
      "|766      |Road-650 Black, 60           |1565.98    |\n",
      "|767      |Road-650 Black, 62           |1565.98    |\n",
      "|768      |Road-650 Black, 44           |1565.98    |\n",
      "|769      |Road-650 Black, 48           |1565.98    |\n",
      "|770      |Road-650 Black, 52           |1565.98    |\n",
      "|775      |Mountain-100 Black, 38       |6749.98    |\n",
      "+---------+-----------------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use where or fliter to do flitering\n",
    "dfProduct.where(dfProduct.Color=='Black') \\\n",
    "         .select(dfProduct.ProductID, dfProduct['Name'], (dfProduct.ListPrice * 2).alias('DoublePrice')) \\\n",
    "         .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------+---------------+\n",
      "|ProductID|Name                     |(ListPrice * 2)|\n",
      "+---------+-------------------------+---------------+\n",
      "|680      |HL Road Frame - Black, 58|2863.0         |\n",
      "|706      |HL Road Frame - Red, 58  |2863.0         |\n",
      "|717      |HL Road Frame - Red, 62  |2863.0         |\n",
      "|718      |HL Road Frame - Red, 44  |2863.0         |\n",
      "|719      |HL Road Frame - Red, 48  |2863.0         |\n",
      "|720      |HL Road Frame - Red, 52  |2863.0         |\n",
      "|721      |HL Road Frame - Red, 56  |2863.0         |\n",
      "|722      |LL Road Frame - Black, 58|674.44         |\n",
      "|723      |LL Road Frame - Black, 60|674.44         |\n",
      "|724      |LL Road Frame - Black, 62|674.44         |\n",
      "|725      |LL Road Frame - Red, 44  |674.44         |\n",
      "|726      |LL Road Frame - Red, 48  |674.44         |\n",
      "|727      |LL Road Frame - Red, 52  |674.44         |\n",
      "|728      |LL Road Frame - Red, 58  |674.44         |\n",
      "|729      |LL Road Frame - Red, 60  |674.44         |\n",
      "|730      |LL Road Frame - Red, 62  |674.44         |\n",
      "|731      |ML Road Frame - Red, 44  |1189.66        |\n",
      "|732      |ML Road Frame - Red, 48  |1189.66        |\n",
      "|733      |ML Road Frame - Red, 52  |1189.66        |\n",
      "|734      |ML Road Frame - Red, 58  |1189.66        |\n",
      "+---------+-------------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfProduct.where(dfProduct.ListPrice * 2 > 100) \\\n",
    "         .select(dfProduct.ProductID, dfProduct['Name'], dfProduct.ListPrice * 2) \\\n",
    "         .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------------+---------+\n",
      "|ProductID|Name                      |ListPrice|\n",
      "+---------+--------------------------+---------+\n",
      "|860      |Half-Finger Gloves, L     |24.49    |\n",
      "|859      |Half-Finger Gloves, M     |24.49    |\n",
      "|858      |Half-Finger Gloves, S     |24.49    |\n",
      "|708      |Sport-100 Helmet, Black   |34.99    |\n",
      "|862      |Full-Finger Gloves, M     |37.99    |\n",
      "|861      |Full-Finger Gloves, S     |37.99    |\n",
      "|863      |Full-Finger Gloves, L     |37.99    |\n",
      "|841      |Men's Sports Shorts, S    |59.99    |\n",
      "|849      |Men's Sports Shorts, M    |59.99    |\n",
      "|851      |Men's Sports Shorts, XL   |59.99    |\n",
      "|850      |Men's Sports Shorts, L    |59.99    |\n",
      "|815      |LL Mountain Front Wheel   |60.745   |\n",
      "|868      |Women's Mountain Shorts, M|69.99    |\n",
      "|869      |Women's Mountain Shorts, L|69.99    |\n",
      "|867      |Women's Mountain Shorts, S|69.99    |\n",
      "|853      |Women's Tights, M         |74.99    |\n",
      "|854      |Women's Tights, L         |74.99    |\n",
      "|852      |Women's Tights, S         |74.99    |\n",
      "|818      |LL Road Front Wheel       |85.565   |\n",
      "|823      |LL Mountain Rear Wheel    |87.745   |\n",
      "+---------+--------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SELECT ProductID, Name, ListPrice \n",
    "# FROM Product \n",
    "# WHERE Color = 'black' \n",
    "# ORDER BY ProductID\n",
    "\n",
    "dfProduct.filter(\"Color = 'Black'\")\\\n",
    "         .select('ProductID', 'Name', 'ListPrice')\\\n",
    "         .orderBy('ListPrice')\\\n",
    "         .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+--------------------+---------+--------+\n",
      "|SalesOrderID|SalesOrderDetailID|                Name|UnitPrice|OrderQty|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "|       71780|            110620|Mountain-500 Blac...|  323.994|       1|\n",
      "|       71780|            110621|LL Mountain Frame...|  149.874|       1|\n",
      "|       71780|            110622|HL Mountain Frame...|   809.76|       1|\n",
      "|       71780|            110623|Mountain-200 Blac...| 1376.994|       4|\n",
      "|       71780|            110627|Women's Mountain ...|   41.994|       6|\n",
      "|       71780|            110629|Mountain-500 Blac...|  323.994|       2|\n",
      "|       71780|            110630|Mountain-500 Blac...|  323.994|       3|\n",
      "|       71780|            110631|Mountain-500 Blac...|  323.994|       1|\n",
      "|       71780|            110632|Mountain-500 Blac...|  323.994|       2|\n",
      "|       71780|            110638|Mountain-200 Blac...| 1376.994|       5|\n",
      "|       71780|            110642|LL Mountain Frame...|  149.874|       1|\n",
      "|       71780|            110643|Women's Mountain ...|   41.994|       7|\n",
      "|       71782|            110690|Sport-100 Helmet,...|   20.994|       7|\n",
      "|       71782|            110697|         HL Crankset|  242.994|       2|\n",
      "|       71782|            110705|Half-Finger Glove...|   14.694|       1|\n",
      "|       71783|            110710|LL Road Frame - B...|  202.332|       4|\n",
      "|       71783|            110712|  Road-250 Black, 44|  1466.01|       3|\n",
      "|       71783|            110713|  Road-750 Black, 58|  323.994|       4|\n",
      "|       71783|            110715|Half-Finger Glove...|   14.694|       7|\n",
      "|       71783|            110725|Half-Finger Glove...|   14.694|       2|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find all orders and details on black product,\n",
    "# return the product SalesOrderID, SalesOrderDetailID, Name, UnitPrice, and OrderQty\n",
    "\n",
    "# SELECT SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty \n",
    "# FROM SalesLT.SalesOrderDetail, SalesLT.Product\n",
    "# WHERE SalesOrderDetail.ProductID = Product.ProductID AND Color = 'Black'\n",
    "\n",
    "# SELECT SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty \n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# JOIN SalesLT.Product ON SalesOrderDetail.ProductID = Product.ProductID\n",
    "# WHERE Color = 'Black'\n",
    "\n",
    "# Spark SQL supports natural joins\n",
    "\n",
    "dfDetail.join(dfProduct, 'ProductID') \\\n",
    "        .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty') \\\n",
    "        .filter(\"Color='Black'\")\\\n",
    "        .show()\n",
    "\n",
    "# If we move the filter to after select, it still works.  Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+--------------------+---------+--------+\n",
      "|SalesOrderID|SalesOrderDetailID|                Name|UnitPrice|OrderQty|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "|       71780|            110620|Mountain-500 Blac...|  323.994|       1|\n",
      "|       71780|            110621|LL Mountain Frame...|  149.874|       1|\n",
      "|       71780|            110622|HL Mountain Frame...|   809.76|       1|\n",
      "|       71780|            110623|Mountain-200 Blac...| 1376.994|       4|\n",
      "|       71780|            110627|Women's Mountain ...|   41.994|       6|\n",
      "|       71780|            110629|Mountain-500 Blac...|  323.994|       2|\n",
      "|       71780|            110630|Mountain-500 Blac...|  323.994|       3|\n",
      "|       71780|            110631|Mountain-500 Blac...|  323.994|       1|\n",
      "|       71780|            110632|Mountain-500 Blac...|  323.994|       2|\n",
      "|       71780|            110638|Mountain-200 Blac...| 1376.994|       5|\n",
      "|       71780|            110642|LL Mountain Frame...|  149.874|       1|\n",
      "|       71780|            110643|Women's Mountain ...|   41.994|       7|\n",
      "|       71782|            110690|Sport-100 Helmet,...|   20.994|       7|\n",
      "|       71782|            110697|         HL Crankset|  242.994|       2|\n",
      "|       71782|            110705|Half-Finger Glove...|   14.694|       1|\n",
      "|       71783|            110710|LL Road Frame - B...|  202.332|       4|\n",
      "|       71783|            110712|  Road-250 Black, 44|  1466.01|       3|\n",
      "|       71783|            110713|  Road-750 Black, 58|  323.994|       4|\n",
      "|       71783|            110715|Half-Finger Glove...|   14.694|       7|\n",
      "|       71783|            110725|Half-Finger Glove...|   14.694|       2|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "*Project [SalesOrderID#227, SalesOrderDetailID#228, Name#181, UnitPrice#231, OrderQty#229]\n",
      "+- *BroadcastHashJoin [ProductID#230], [ProductID#180], Inner, BuildRight\n",
      "   :- *Project [SalesOrderID#227, SalesOrderDetailID#228, OrderQty#229, ProductID#230, UnitPrice#231]\n",
      "   :  +- *Filter isnotnull(ProductID#230)\n",
      "   :     +- *FileScan csv [SalesOrderID#227,SalesOrderDetailID#228,OrderQty#229,ProductID#230,UnitPrice#231] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/csproject/msbd5003/public_html/data/SalesOrderDetail.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<SalesOrderID:int,SalesOrderDetailID:int,OrderQty:int,ProductID:int,UnitPrice:double>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n",
      "      +- *Project [ProductID#180, Name#181]\n",
      "         +- *Filter ((isnotnull(Color#183) && (Color#183 = Black)) && isnotnull(ProductID#180))\n",
      "            +- *FileScan csv [ProductID#180,Name#181,Color#183] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/csproject/msbd5003/public_html/data/Product.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Color), EqualTo(Color,Black), IsNotNull(ProductID)], ReadSchema: struct<ProductID:int,Name:string,Color:string>\n"
     ]
    }
   ],
   "source": [
    "# This also works:\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID') \\\n",
    "             .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty')\n",
    "d2 = d1.filter(\"Color = 'Black'\")\n",
    "d2.show()\n",
    "d2.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|SalesOrderID|\n",
      "+------------+\n",
      "|       71902|\n",
      "|       71832|\n",
      "|       71915|\n",
      "|       71831|\n",
      "|       71898|\n",
      "|       71935|\n",
      "|       71938|\n",
      "|       71845|\n",
      "|       71783|\n",
      "|       71815|\n",
      "|       71936|\n",
      "|       71863|\n",
      "|       71780|\n",
      "|       71782|\n",
      "|       71899|\n",
      "|       71784|\n",
      "|       71797|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find all orders that include at least one black product, \n",
    "# return the product SalesOrderID, Name, UnitPrice, and OrderQty\n",
    "\n",
    "# SELECT DISTINCT SalesOrderID\n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# JOIN SalesLT.Product ON SalesOrderDetail.ProductID = Product.ProductID\n",
    "# WHERE Color = 'Black'\n",
    "\n",
    "dfDetail.join(dfProduct.filter(\"Color='Black'\"), 'ProductID') \\\n",
    "        .select('SalesOrderID') \\\n",
    "        .distinct() \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many colors in the products?\n",
    "\n",
    "# SELECT COUNT(DISTINCT Color)\n",
    "# FROM SalesLT.Product\n",
    "\n",
    "dfProduct.select('Color').distinct().count()\n",
    "\n",
    "# It's 1 more than standard SQL.  In standard SQL, COUNT() does not count NULLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|SalesOrderID|        TotalPrice|\n",
      "+------------+------------------+\n",
      "|       71867|             858.9|\n",
      "|       71902|59894.209199999976|\n",
      "|       71832|      28950.678108|\n",
      "|       71915|1732.8899999999999|\n",
      "|       71946|            31.584|\n",
      "|       71895|221.25600000000003|\n",
      "|       71816|2847.4079999999994|\n",
      "|       71831|          1712.946|\n",
      "|       71923|         96.108824|\n",
      "|       71858|11528.844000000001|\n",
      "|       71917|            37.758|\n",
      "|       71897|          10585.05|\n",
      "|       71885|           524.664|\n",
      "|       71856|500.30400000000003|\n",
      "|       71898| 53248.69200000002|\n",
      "|       71774|           713.796|\n",
      "|       71796| 47848.02600000001|\n",
      "|       71935|5533.8689079999995|\n",
      "|       71938|         74160.228|\n",
      "|       71845|        34118.5356|\n",
      "+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the total price of each order, \n",
    "# return SalesOrderID and total price (column name should be ‘totalprice’)\n",
    "\n",
    "# SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice\n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# GROUP BY SalesOrderID\n",
    "\n",
    "dfDetail.select('*', (dfDetail.UnitPrice * dfDetail.OrderQty\n",
    "                      * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    "        .groupBy('SalesOrderID').sum('netprice') \\\n",
    "        .withColumnRenamed('sum(netprice)', 'TotalPrice')\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|SalesOrderID|        TotalPrice|\n",
      "+------------+------------------+\n",
      "|       71902|59894.209199999976|\n",
      "|       71832|      28950.678108|\n",
      "|       71858|11528.844000000001|\n",
      "|       71897|          10585.05|\n",
      "|       71898| 53248.69200000002|\n",
      "|       71796| 47848.02600000001|\n",
      "|       71938|         74160.228|\n",
      "|       71845|        34118.5356|\n",
      "|       71783|      65683.367986|\n",
      "|       71936| 79589.61602399996|\n",
      "|       71780|29923.007999999998|\n",
      "|       71782| 33319.98600000001|\n",
      "|       71784| 89869.27631400003|\n",
      "|       71797| 65123.46341800001|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the total price of each order where the total price > 10000\n",
    "\n",
    "# SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice\n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# GROUP BY SalesOrderID\n",
    "# HAVING SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) > 10000\n",
    "\n",
    "dfDetail.select('*', (dfDetail.UnitPrice * dfDetail. OrderQty\n",
    "                      * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    "        .groupBy('SalesOrderID').sum('netprice') \\\n",
    "        .withColumnRenamed('sum(netprice)', 'TotalPrice')\\\n",
    "        .where('TotalPrice > 10000')\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------+\n",
      "|SalesPerson           |sum(TotalPrice)   |\n",
      "+----------------------+------------------+\n",
      "|adventure-works\\linda3|170064.83797      |\n",
      "|adventure-works\\shu0  |112646.20642399996|\n",
      "|adventure-works\\jae0  |425979.108664     |\n",
      "+----------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "details=dfDetail.select('*', (dfDetail.UnitPrice * dfDetail. OrderQty\n",
    "         * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    "        .join(dfProduct, 'ProductID') \\\n",
    "        .groupBy('SalesOrderID').sum('netprice') \\\n",
    "        .withColumnRenamed('sum(netprice)', 'TotalPrice')\n",
    "        \n",
    "result=dfHeader.join(details,'SalesOrderID','left_outer')\\\n",
    "    .join(dfCustomer,\"CustomerID\",'left_outer')\\\n",
    "    .groupBy('SalesPerson').sum('TotalPrice').na.fill(0)\n",
    "    \n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|SalesOrderID|        TotalPrice|\n",
      "+------------+------------------+\n",
      "|       71902|         26677.884|\n",
      "|       71832|      16883.748108|\n",
      "|       71938|         33779.448|\n",
      "|       71845|         18109.836|\n",
      "|       71783|15524.117476000003|\n",
      "|       71936| 44490.29042399999|\n",
      "|       71780|16964.321999999996|\n",
      "|       71797|      27581.613792|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the total price on the black products of each order where the total price > 10000\n",
    "\n",
    "# SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice\n",
    "# FROM SalesLT.SalesOrderDetail, SalesLT.Product\n",
    "# WHERE SalesLT.SalesOrderDetail.ProductID = SalesLT.Product.ProductID AND Color = 'Black'\n",
    "# GROUP BY SalesOrderID\n",
    "# HAVING SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) > 10000\n",
    "\n",
    "dfDetail.select('*', (dfDetail.UnitPrice * dfDetail. OrderQty\n",
    "                      * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    "        .join(dfProduct.where(\"Color = 'Black'\"), 'ProductID') \\\n",
    "        .groupBy('SalesOrderID').sum('netprice') \\\n",
    "        .withColumnRenamed('sum(netprice)', 'TotalPrice')\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+-------------+\n",
      "|CustomerID|   FirstName|    LastName|sum(OrderQty)|\n",
      "+----------+------------+------------+-------------+\n",
      "|     30050|     Krishna|Sunkammurali|           89|\n",
      "|     29796|         Jon|      Grande|           65|\n",
      "|     29957|       Kevin|         Liu|           62|\n",
      "|     29929|     Jeffrey|       Kurtz|           46|\n",
      "|     29546| Christopher|        Beck|           45|\n",
      "|     29922|      Pamala|        Kotc|           34|\n",
      "|     30113|        Raja|   Venugopal|           34|\n",
      "|     29938|       Frank|    Campbell|           29|\n",
      "|     29736|       Terry|   Eminhizer|           23|\n",
      "|     29485|   Catherine|        Abel|           10|\n",
      "|     30019|     Matthew|      Miller|            9|\n",
      "|     29932|     Rebecca|      Laszlo|            7|\n",
      "|     29975|      Walter|        Mays|            5|\n",
      "|     29638|    Rosmarie|     Carroll|            2|\n",
      "|     30089|Michael John|      Troyer|            1|\n",
      "|     29568|      Donald|     Blanton|            1|\n",
      "|     29531|        Cory|       Booth|            1|\n",
      "+----------+------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each customer, find the total quantity of black products bought.\n",
    "# Report CustomerID, FirstName, LastName, and total quantity\n",
    "\n",
    "# select saleslt.customer.customerid, FirstName, LastName, sum(orderqty)\n",
    "# from saleslt.customer\n",
    "# left outer join \n",
    "# (\n",
    "# saleslt.salesorderheader\n",
    "# join saleslt.salesorderdetail\n",
    "# on saleslt.salesorderdetail.salesorderid = saleslt.salesorderheader.salesorderid\n",
    "# join saleslt.product\n",
    "# on saleslt.product.productid = saleslt.salesorderdetail.productid and color = 'black'\n",
    "# )\n",
    "# on saleslt.customer.customerid = saleslt.salesorderheader.customerid\n",
    "# group by saleslt.customer.customerid, FirstName, LastName\n",
    "# order by sum(orderqty) desc\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID')\\\n",
    "             .where('Color = \"Black\"') \\\n",
    "             .join(dfHeader, 'SalesOrderID')\\\n",
    "             .groupBy('CustomerID').sum('OrderQty')\n",
    "dfCustomer.join(d1, 'CustomerID', 'left_outer')\\  \n",
    "          .select('CustomerID', 'FirstName', 'LastName', 'sum(OrderQty)')\\\n",
    "          .orderBy('sum(OrderQty)', ascending=False)\\\n",
    "          .show()   # left-outer join!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "### Embed SQL queries\n",
    "\n",
    "You can also run SQL queries over dataframes once you register them as temporary tables within the SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Register the dataframe as a temporary view called HVAC\n",
    "df.createOrReplaceTempView('HVAC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+------------+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|     Country|\n",
      "+----------+-----------+-----------+-----------+------------+\n",
      "|         1|         M1|         25|     AC1000|         USA|\n",
      "|         2|         M2|         27|     FN39TG|      France|\n",
      "|         3|         M3|         28|     JDNS77|      Brazil|\n",
      "|         4|         M4|         17|     GG1919|     Finland|\n",
      "|         7|         M7|         13|     FN39TG|South Africa|\n",
      "|         8|         M8|         25|     JDNS77|   Australia|\n",
      "|         9|         M9|         11|     GG1919|      Mexico|\n",
      "|        10|        M10|         23|    ACMAX22|       China|\n",
      "|        11|        M11|         14|     AC1000|     Belgium|\n",
      "|        12|        M12|         26|     FN39TG|     Finland|\n",
      "|        13|        M13|         25|     JDNS77|Saudi Arabia|\n",
      "|        14|        M14|         17|     GG1919|     Germany|\n",
      "|        15|        M15|         19|    ACMAX22|      Israel|\n",
      "|        16|        M16|         23|     AC1000|      Turkey|\n",
      "|        17|        M17|         11|     FN39TG|       Egypt|\n",
      "|        18|        M18|         25|     JDNS77|   Indonesia|\n",
      "|        19|        M19|         14|     GG1919|      Canada|\n",
      "|        20|        M20|         19|    ACMAX22|   Argentina|\n",
      "+----------+-----------+-----------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM HVAC WHERE BuildingAge >= 10').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|HVACproduct|count(1)|\n",
      "+-----------+--------+\n",
      "|    ACMAX22|       3|\n",
      "|     AC1000|       3|\n",
      "|     JDNS77|       4|\n",
      "|     FN39TG|       4|\n",
      "|     GG1919|       4|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Can even mix DataFrame API with SQL:\n",
    "df.where('BuildingAge >= 10').createOrReplaceTempView('OldBuildings')\n",
    "spark.sql('SELECT HVACproduct, COUNT(*) FROM OldBuildings GROUP BY HVACproduct').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|HVACproduct|count|\n",
      "+-----------+-----+\n",
      "|    ACMAX22|    3|\n",
      "|     AC1000|    3|\n",
      "|     JDNS77|    4|\n",
      "|     FN39TG|    4|\n",
      "|     GG1919|    4|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d1 = spark.sql('SELECT * FROM HVAC WHERE BuildingAge >= 10')\n",
    "d1.groupBy('HVACproduct').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|     Country|slen|\n",
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "|         1|         M1|         25|     AC1000|         USA|   5|\n",
      "|         2|         M2|         27|     FN39TG|      France|   8|\n",
      "|         3|         M3|         28|     JDNS77|      Brazil|   8|\n",
      "|         4|         M4|         17|     GG1919|     Finland|   9|\n",
      "|         5|         M5|          3|    ACMAX22|   Hong Kong|  11|\n",
      "|         6|         M6|          9|     AC1000|   Singapore|  11|\n",
      "|         7|         M7|         13|     FN39TG|South Africa|  14|\n",
      "|         8|         M8|         25|     JDNS77|   Australia|  11|\n",
      "|         9|         M9|         11|     GG1919|      Mexico|   8|\n",
      "|        10|        M10|         23|    ACMAX22|       China|   7|\n",
      "|        11|        M11|         14|     AC1000|     Belgium|   9|\n",
      "|        12|        M12|         26|     FN39TG|     Finland|   9|\n",
      "|        13|        M13|         25|     JDNS77|Saudi Arabia|  14|\n",
      "|        14|        M14|         17|     GG1919|     Germany|   9|\n",
      "|        15|        M15|         19|    ACMAX22|      Israel|   8|\n",
      "|        16|        M16|         23|     AC1000|      Turkey|   8|\n",
      "|        17|        M17|         11|     FN39TG|       Egypt|   7|\n",
      "|        18|        M18|         25|     JDNS77|   Indonesia|  11|\n",
      "|        19|        M19|         14|     GG1919|      Canada|   8|\n",
      "|        20|        M20|         19|    ACMAX22|   Argentina|  11|\n",
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UDF\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "slen = udf(lambda s: len(s)+2, IntegerType())\n",
    "df.select('*', slen(df['Country']).alias('slen')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|     Country|slen|\n",
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "|         1|         M1|         25|     AC1000|         USA|   3|\n",
      "|         2|         M2|         27|     FN39TG|      France|   6|\n",
      "|         3|         M3|         28|     JDNS77|      Brazil|   6|\n",
      "|         4|         M4|         17|     GG1919|     Finland|   7|\n",
      "|         5|         M5|          3|    ACMAX22|   Hong Kong|   9|\n",
      "|         6|         M6|          9|     AC1000|   Singapore|   9|\n",
      "|         7|         M7|         13|     FN39TG|South Africa|  12|\n",
      "|         8|         M8|         25|     JDNS77|   Australia|   9|\n",
      "|         9|         M9|         11|     GG1919|      Mexico|   6|\n",
      "|        10|        M10|         23|    ACMAX22|       China|   5|\n",
      "|        11|        M11|         14|     AC1000|     Belgium|   7|\n",
      "|        12|        M12|         26|     FN39TG|     Finland|   7|\n",
      "|        13|        M13|         25|     JDNS77|Saudi Arabia|  12|\n",
      "|        14|        M14|         17|     GG1919|     Germany|   7|\n",
      "|        15|        M15|         19|    ACMAX22|      Israel|   6|\n",
      "|        16|        M16|         23|     AC1000|      Turkey|   6|\n",
      "|        17|        M17|         11|     FN39TG|       Egypt|   5|\n",
      "|        18|        M18|         25|     JDNS77|   Indonesia|   9|\n",
      "|        19|        M19|         14|     GG1919|      Canada|   6|\n",
      "|        20|        M20|         19|    ACMAX22|   Argentina|   9|\n",
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register('slen', lambda s: len(s), IntegerType())\n",
    "spark.sql('SELECT *, slen(Country) AS slen FROM HVAC').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Flexible Data Model\n",
    "\n",
    "Sample data file at\n",
    "\n",
    "https://www.cse.ust.hk/msbd5003/data/products.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dimensions: struct (nullable = true)\n",
      " |    |-- height: double (nullable = true)\n",
      " |    |-- length: double (nullable = true)\n",
      " |    |-- width: double (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- warehouseLocation: struct (nullable = true)\n",
      " |    |-- latitude: double (nullable = true)\n",
      " |    |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json('../data/products.json')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+----------------+-----+-----------+-----------------+\n",
      "|    dimensions| id|            name|price|       tags|warehouseLocation|\n",
      "+--------------+---+----------------+-----+-----------+-----------------+\n",
      "|[9.5,7.0,12.0]|  2|An ice sculpture| 12.5|[cold, ice]|    [-78.75,20.4]|\n",
      "| [1.0,3.1,1.0]|  3|    A blue mouse| 25.5|       null|     [54.4,-32.7]|\n",
      "+--------------+---+----------------+-----+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|height|\n",
      "+------+\n",
      "|   9.5|\n",
      "|   1.0|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Accessing nested fields\n",
    "\n",
    "df.select(df['dimensions.height']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|height|\n",
      "+------+\n",
      "|   9.5|\n",
      "|   1.0|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('dimensions.height').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|height|\n",
      "+------+\n",
      "|   9.5|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('dimensions.height')\\\n",
    "  .filter(\"tags[0] = 'cold' AND warehouseLocation.latitude < 0\")\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dimensions=Row(height=9.5, length=7.0, width=12.0), id=2, name=u'An ice sculpture', price=12.5, tags=[u'cold', u'ice'], warehouseLocation=Row(latitude=-78.75, longitude=20.4)), Row(dimensions=Row(height=1.0, length=3.1, width=1.0), id=3, name=u'A blue mouse', price=25.5, tags=None, warehouseLocation=Row(latitude=54.4, longitude=-32.7))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Converting between RDD and DataFrame\n",
    "\n",
    "Sample data file at:\n",
    "\n",
    "https://www.cse.ust.hk/msbd5003/data/people.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Michael', 29), (u'Andy', 30), (u'Justin', 19)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"../data/people.txt\")\n",
    "\n",
    "def parse(l):\n",
    "    a = l.split(',')\n",
    "    return (a[0], int(a[1]))\n",
    "\n",
    "rdd = lines.map(parse)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+-------+---+\n",
      "|     _1| _2|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame from an RDD of tuples, schema is inferred\n",
    "df = spark.createDataFrame(rdd)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n",
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame from an RDD of tuples with column names, type is inferred\n",
    "df = spark.createDataFrame(rdd, ['name', 'age'])\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 29|Michael|\n",
      "| 30|   Andy|\n",
      "| 19| Justin|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame from an RDD of Rows, type is given in the Row objects\n",
    "from pyspark.sql import Row\n",
    "\n",
    "rdd_rows = rdd.map(lambda p: Row(name = p[0], age = p[1]))\n",
    "df = spark.createDataFrame(rdd_rows)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age| name|\n",
      "+----+-----+\n",
      "|  11|Alice|\n",
      "|null|  Bob|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Row fields with types incompatible with that of previous rows will be turned into nulls\n",
    "row1 = Row(name=\"Alice\", age=11)\n",
    "row2 = Row(name=\"Bob\", age='12')\n",
    "rdd_rows = sc.parallelize([row1, row2])\n",
    "df1 = spark.createDataFrame(rdd_rows)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Name: Justin']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd returns the content as an RDD of Rows\n",
    "teenagers = df.filter('age >= 13 and age <= 19')\n",
    "\n",
    "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name)\n",
    "teenNames.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "DataFrames are stored using columnar storage with compression\n",
    "\n",
    "RDDs are stored using row storage without compression\n",
    "\n",
    "The RDD view of DataFrame just provides an interface, the Row objects are constructed on the fly and do not necessarily represent the internal storage format of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closure in DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  0|  0|\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "|  3|  3|\n",
      "|  4|  4|\n",
      "|  5|  5|\n",
      "|  6|  6|\n",
      "|  7|  7|\n",
      "|  8|  8|\n",
      "|  9|  9|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = range(10)\n",
    "df = spark.createDataFrame(zip(data, data))\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  0|  0|\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "|  3|  3|\n",
      "|  4|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The 'closure' behaviour in RDD doesn't seem to exist for DataFrames\n",
    "\n",
    "x = 5\n",
    "df1 = df.filter(df._1 < x)\n",
    "#df1.show()\n",
    "x = 3\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Filter (isnotnull(_1#773L) && (_1#773L < 5))\n",
      "+- Scan ExistingRDD[_1#773L,_2#774L]\n"
     ]
    }
   ],
   "source": [
    "# Because of the Catalyst optimizer !\n",
    "\n",
    "df1.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|((((_1 * 2) + 2) + 1) + 1)|\n",
      "+--------------------------+\n",
      "|                         4|\n",
      "|                         6|\n",
      "|                         8|\n",
      "|                        10|\n",
      "|                        12|\n",
      "|                        14|\n",
      "|                        16|\n",
      "|                        18|\n",
      "|                        20|\n",
      "|                        22|\n",
      "+--------------------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*Project [((_1#1664L * 2) + 4) AS ((((_1 * 2) + 2) + 1) + 1)#1682L]\n",
      "+- Scan ExistingRDD[_1#1664L,_2#1665L]\n"
     ]
    }
   ],
   "source": [
    "def f():\n",
    "    return x/2\n",
    "x = 5\n",
    "df1 = df.select(df._1 * 2 + f() + 1 + 1)\n",
    "df.select(df._1 * 2 + f() + 1 + 1).show()\n",
    "df1.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(10))\n",
    "x = 5\n",
    "a = rdd.filter(lambda z: z < x)\n",
    "print a.take(10)\n",
    "x = 3\n",
    "print a.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "def increment_counter(x):\n",
    "    global counter\n",
    "    counter += 1\n",
    "\n",
    "df.foreach(increment_counter)\n",
    "\n",
    "print counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
